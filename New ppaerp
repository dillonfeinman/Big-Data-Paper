







Apache Spark: The New and Improved Hadoop MapReduce
Dillon Feinman
Binghamton University
 
Abstract
This paper argues that Apache Spark is the superior data-analysis software to Apache Hadoop MapReduce. First, I discuss the programming model for both Spark and MapReduce along with the benefits to each of them. After giving an introduction to both of the programs, experiments are used to compare the two. The first experiment assessed the execution times of analyzing log files, and the second evaluated the usability of each program for new users. These experiments both favored Spark over MapReduce, mainly due to the superior graphical user interface and the fact that it executes in memory rather than disc. After comparing the two, I discuss if storing a large amount of personal information is ethical. I believe that although there are ethical reasons for storing massive amounts of personal data, like the government does to protect the public, most organizations using large databases tend to focus on financial motive and legality over ethicality. 
Apache Spark: The New and Improved Hadoop MapReduce
Introduction
With more technological advances created every day around the world, billions of devices have been connected to the Internet. These devices collect data to later analyze in order to advance their technologies. With all of this data, companies can do wonderful things such as send customers personalized ads or suggest videos that the viewer would most likely enjoy, but how do these companies analyze this data? There are two main frameworks: Apache Hadoop MapReduce and Apache Spark. Since Apache Spark can handle different datasets at a faster rate, it is a better choice than the popular Hadoop MapReduce, even though the latter can handle much larger datasets. There are a number of experiments that prove that Spark is the faster option and provides a more user-friendly experience than its counterpart, Hadoop MapReduce. For example, researchers looked at different log files containing a different number of slave nodes, different size files, and different applications of the files to see which framework handled the data faster. Another experiment consisted of researchers teaching a class how to use both of these data-analyzing frameworks and having the students decide which one they were more likely to use. Both Hadoop MapReduce and Apache Spark are viable options to data storage and analysis, but with more types of data becoming available, speed and universality are increasingly important factors.
Background
	Currently, computer scientists are able to collect more data than ever before. This huge amount of data, often called big data, can be about anything, such as a user’s Google history, where a user’s mouse is on the screen, or what advertisements a user clicks on. This type of data may seem pointless to collect, but with this information, companies are able to learn a lot about the user. For example, the way a user moves his mouse may correlate to where he is looking on the screen. If the website developer knows where users are looking most often, he can adjust the website in order to put important content in an area where more people are likely to focus on. Likewise, if the website creator knows which advertisements a user often clicks on, he will be able to send specific advertisements that may be more attractive to that user. 
Another benefit to big data is the aid to law enforcement. New data-analysis frameworks, such as Spark, are decreasing execution times of analyzing data. There are over 330 million people in the United States. If the government needs to search through every person’s records in a life or death situation, like a terrorist attack, then that search needs to be fast. Big data has helped stop many criminals, some even before they acted. There are obviously many benefits to big data, but these benefits come with a loss of privacy.
Although big data has its advantages, the storage of large amounts of information can result in a number of negative consequences. For example, it can result in legal issues among privacy advocates and businesses. In the United States, citizens have a right to privacy granted by the Fourth Amendment. The storage of information about practically everything the user does on the Internet can be seen as infringing on the user’s freedom of privacy. Companies also abuse people’s privacy rights by misusing the information they receive. Companies with access to people’s personal information, such as Facebook, have sold their users’ information for financial gain. This is a problem because other companies will then have this data, which is difficult to retrieve and delete. 
Company databases, specifically those of small companies, are also prone to hackers. Hackers are people who try to get around imposed limits to gain access to places they don’t have the authorization to go to. There have been many data breaches that resulted in hackers acquiring the personal information of millions of individuals. A specific example is the Target Breach. In 2013, Target had its records of over 41 million credit cards breached by hackers (McCoy, para.1). If companies didn’t store this much data, hackers might be less interested in accessing these databases; however, 41 million different payment methods in one location is very enticing to hackers. This also begs the question of whether or not it should be legal for companies to store personal information for as long as they want. Hackers are creating new methods of breaking into databases every day. Because of this, companies’ online data will never be completely safe. One could argue that if companies cannot protect their users’ data, then maybe they should not be allowed to store this information. 
With hackers and companies tracking practically everything one does online, people are struggling to retain the right to be forgotten. Also, in other circumstances, users are being recorded without their knowledge (Baase & Henry, p.75). If one does not know they are being recorded, or they do not know where their information is, it is practically impossible to attempt to have this information removed from the web. Companies usually put what they intend to do with the information in their terms of use, but most people do not read this contract. Companies should explicitly state what information they are recording, and what they are going to do with that information. People should be allowed to keep their personal information private if that is what they want, but new technology, especially big data, is hurting our privacy. Despite these consequences, companies will continue to collect large amounts of data on their users in order to improve themselves, but how exactly do these companies analyze this information?


Precedents/Related Work
Hadoop MapReduce and Apache Spark
Companies use data-analyzing software in order to study the massive amounts of data they receive. For a long time, Apache Hadoop MapReduce was the most viable option for companies to use. In their paper “The Family of MapReduce and Large-Scale Data Processing Systems” (2013), Liu, Sakr, and Fayoumi discuss a number of reasons why a company would choose Hadoop MapReduce. Prior to Hadoop MapReduce, there were no free options for storing and analyzing data. They state that the most obvious reason people choose Hadoop MapReduce is because it is an open source project, meaning that it is free to download and use (p.2), and has been around the longest, giving it time to amass a positive reputation. Other reasons why people use MapReduce is due to its ability to handle faults. If one of the computing systems, commonly called nodes, were to crash, MapReduce would be able to continue functioning without it until someone is able to fix the node. There are plenty of advantageous reasons to use Hadoop MapReduce, however it is not the best option today.
Introduction to Hadoop MapReduce. Before discussing the alternatives to Hadoop MapReduce, one must understand the benefits of MapReduce and how it works. Liu, Sakr, and Fayoumi (2013) describe MapReduce as a “simple and powerful programming model that enables easy development of scalable parallel applications to process vast amounts of data on large clusters of commodity machines” (p.2). Essentially, this means that MapReduce allows individuals that are not the most technologically advanced to develop applications that use a number of computing components to read and analyze huge amounts of data. 
The authors say that in order to analyze this data, the software uses computations that take a key and value pair as an input and organizes it in a different key and value pair (p.2). These computations are set up by the user of MapReduce in two different steps, a mapping function and a reducing function. The mapping function does exactly that; it maps the key and value pair to an intermediate key and value. After that mapping stage, the program sends all intermediate values paired with a certain intermediate key to the reducing function, which merges the intermediate values together and associates a single value to the key sent to the reducing function. Converting the massive amount of values into a single value allows for large computations to be re-executed and parallelized easily. This data-analysis model provides a number of benefits.
Hadoop MapReduce Benefits. There are a number of obvious advantages to Hadoop MapReduce. Besides being free software, it reduces cost to the company using it. Since MapReduce allows the parallelization of multiple computers, the user can purchase “low-cost, unreliable hardware” (p.2). The authors state that users can buy many cheap and unreliable computers, commonly called nodes, and combine each of their processing powers to act as one very powerful machine. Since MapReduce uses many nodes at once, it is fault tolerant. This means that if one node has a problem, the other nodes can absorb the workload from the one broken node. Liu, Sakr, and Fayoumi also state that data-parallelization allows users to not be concerned about the low-level details of the problem (p.2). This means that instead of worrying about how memory is allocated, or other issues not directly associated with the specific problem, the user can simply focus on the goal of the program.
	Introduction to Spark. In Zaharia et al.’s article “Apache Spark: A Unified Engine for Big Data Processing” (2016), the authors, a group of researchers at the University of California, Berkley, who created Apache Spark, discuss their reasons for inventing this software. Before Spark, there were many specialized frameworks that handled specific types of workloads. For example, the authors state that “MapReduce supported batch processing…Dremel for interactive SQL queries and Pregel for iterative graph algorithms” (p.56). The authors decided that requiring different frameworks is inefficient; this model requires the user to download or purchase many different pieces of software and use them at the same time. Their goal was to develop a singular engine that can process many different types of workloads (p.56). While Spark is similar to Hadoop MapReduce, it is different in the sense that it contains Resilient Distributed Datasets (RDDs). This addition allows for the user to process workloads that usually required different engines. For example, Spark includes “SQL, streaming, machine learning, and graph processing” (p.56). The authors report that Spark’s processing for specific workloads had similar performance to the original engine.
	Benefits to Spark. Spark has many of the same benefits as Hadoop MapReduce. For example, Spark is also free software. In addition, Spark also allows users to create applications with more ease than MapReduce. This is because Spark uses a unified API (p.57). This means that the user does not need to create unique programs for different engines. Also, Spark can use the data in a different engine without needing to send it to storage. Instead, Spark can send the data to memory to use in different types of functions. Not having to write to disc is an important aspect of Spark. Not having to send to storage, like MapReduce does, allows Spark to operate much faster than MapReduce. Spark can also handle large datasets, like MapReduce. Spark uses all of its memory space prior to writing to the disk. Although the two frameworks function similarly, Spark is the better choice between the two, as is seen in the multiple experiments conducted by researchers.


Support
Comparing MapReduce and Spark
	MapReduce and Spark both follow similar programming models, however, the way they operate is different. For example, Spark writes to memory and operates faster, while MapReduce writes to storage at a slower rate. Also, Spark is one unified engine, while MapReduce needs a number of engines, depending on the task the user wants to complete. It seems obvious that Spark would be the preferable piece of software, and to further prove this, there are experiments that were conducted by professionals which demonstrate various points where Spark outperforms MapReduce.
	Log File Analysis. Researchers from the Aristotle University of Thessaloniki in Greece, Mavridis and Karatza, wrote a paper called “Performance evaluation of cloud-based log file 
analysis with Apache Hadoop and Apache Spark” (2016). In this paper, they discuss results of their experiment in which they analyzed “log data in order to extract some useful information” (para.22). In order to complete this experiment, they first created a cloud containing log files from different websites (para.54). They created a system containing six nodes; this consisted of one master node and five slave nodes. They then collected the time of execution for different log files (para.61). The researchers used log files of size 1.1GB, 5.5GB, and 11GB in their experiment. They found that Hadoop MapReduce’s execution times (at the highest) are 250 seconds, 1250 seconds, and 2500 seconds respectively (para.72). In comparison, Spark’s (highest) execution times came to 55 seconds, 155 seconds, and 325 seconds, respectively (para.76). This experiment proves that Spark operates faster than MapReduce.
	Usability. Akil, Zhou, and Röhm analyzed the usability of both Hadoop MapReduce and Apache Spark in their paper Technical Report: On the Usability of Hadoop 
MapReduce, Apache Spark & Apache Flink for Data Science. They conducted an experiment at the University of  Sydney, Australia. This experiment consisted of teaching a cloud computing class how to use both MapReduce and Spark (p.3). After learning each of the pieces of software, the students were given different assignments using each program. After all of the assignments were completed, the majority of the students decided that they preferred Apache Spark over Hadoop MapReduce (p.27). The users stated that Spark is easier to use, since it is a newer and higher-level program, opposed to the older and lower-level MapReduce. It is obvious to see that Spark is the better data-analysis software, but using these pieces of software to store and analyze all of this data may be considered unethical.
Social Impact of Big Data
	Social Impact. Whether used ethically or unethically, big data will impact the majority of individuals’ lives. Many big companies, such as Google, Facebook, and YouTube, use big data to track their customers’ online activity. Most of the time, the use of data is beneficial to the user. As discussed in the background section, these companies track their users’ activity in order to improve their experience.
	Google is an example of a company that uses data to improve user experience. Google records every single search query, and uses this to predict what the user will look up. When a major event occurs, say the Super Bowl, the top search suggestion would most likely be “Super Bowl outcome,” or something along those lines. Google also records the websites a user visits, and the online auctions a user views. It uses this information to send advertisements for products it believes the user would be interested in purchasing. Some people would see this type of tracking as a violation of one’s privacy, but others see it as a specialized experience.
The government also tracks people’s online activity. This monitoring greatly helps law enforcement catch criminals, both physical and cyber.
Ethical Analysis of Big Data
Legality of Big Data. One might believe that the amount of information companies collect about individuals should be illegal, however, this is not the case. These companies are actually following the law because they usually have a terms of service contract. Terms of service contracts are legal documents that state what the user agrees to when utilizing a company’s service. In this document, the company usually states what information they are collecting and how they are using it. This practice allows companies to exploit the consumers’ data. The main problem with terms of service contracts is that most people skip reading them. These documents are usually long and contain small font, which seems to dissuade people from reading them. Since most people do not know what data a company is collecting, nor what they are doing with this data, they tend to lose track of the information they have given out. If they do not know who has what information, it becomes impossible to attempt to remove their private information from online databases.
Right to Be Forgotten. Companies should consider the personal information they are given and the data they receive to be confidential, much like information given to a lawyer or doctor. Unfortunately, however, this is atypical; companies usually sell people’s personal information to other companies. The companies purchasing this personal data use this information to find potential customers. Individuals should be able to ask a company to delete their personal data and that company should follow this request. Unfortunately, this tends to get more challenging with every sale of data, because with so many businesses involved, there are likely to be many companies that the individuals don’t even know exist. This makes it extremely difficult for individuals to maintain their personal information. 
Ethical Uses of Big Data. There are plenty of examples where collecting data about individuals is considered ethical. For example, the government collects data on everyone in their country in order to help protect their people. If one is arrested, the police collect the individual’s fingerprints and mugshot, and this data is kept in government databases. Federal and state officials use these databases to help identify people that have broken the law. Another example of the federal government ethically using big data is the National Security Agency (NSA) reading the public’s messages. The NSA is a federal organization designed to protect the general public from terrorism. The NSA takes in huge amounts of data and flags individuals that may be participating in terroristic activities. The storing of private information by the government to help the general public is an ethical example of the storage and analysis of private information, but these are rare examples. Most private companies are only focused on the financials, and not so much the ethicality.
Conclusion
	There are over a million terabytes of information stored in databases on the Internet and on personal computers right now, and there is more data collected every day. This huge amount of data calls for new ways to store and analyze this information in order to increase efficiency. Apache Hadoop MapReduce, an older and commonly used data-analysis program, and Apache Spark, a newer and higher-level data-analysis software were compared to display the difference in newer data-analyzing software. The way both of these pieces of software operate were discussed, as well as the benefits to each of them. Experiments done at many different universities were utilized in order to see the difference between the two frameworks. The first one evaluated execution times of analyzing log files for Hadoop MapReduce and Spark. The researchers found that Spark executed much faster in every scenario. The second experiment compared the usability of each program. The usability experiment consisted of teaching a class of students both MapReduce and Spark, and then had them choose which software they were more likely to use. An overwhelming majority chose Spark over MapReduce as it can be used for many different tasks and had a superior graphical user interface. It is obvious to see that Spark is a better option than MapReduce. Also, the ethical status of using these programs to store and analyze data was determined. The storage of this data can be used ethically, but in many cases, companies do not consider the ethics of their actions; they only consider the legality. The government needs to pass laws demanding that companies clearly state their intentions when using people’s private data.
 
References
Akil, B., Zhou, Y., & Röhm, U. (2018). Technical Report: On the Usability of Hadoop 
MapReduce, Apache Spark & Apache Flink for Data Science. Retrieved from https://bingprimo.hosted.exlibrisgroup.com/primo-explore/fulldisplay?docid=TN_arxiv1803.10836&context=PC&vid=BING&lang=en_US&search_scope=Everything_Scope&adaptor=primo_central_multiple_fe&tab=default_tab&query=any,contains,Hadoop%20Mapreduce,AND&query=any,contains,Spark,AND&sortby=rank&facet=rtype,include,articles&mode=advanced&offset=0
Baase, S., & Henry, T.M. (2018). A Gift of Fire: Social, Legal, and Ethical Issues for Computing 
Technology. Hoboken, NJ. Pearson.
Liu, A., Sakr, S., & Fayoumi, A. G. (2013, October). The Family of MapReduce and Large-Scale 
Data Processing Systems. ACM Computing Surveys, 46(1).
Mavridis, I., & Karatza, H. (2017, March). Performance evaluation of cloud-based log file 
analysis with Apache Hadoop and Apache Spark. The Journal of Systems & Software, 125, 133-151.
McCoy, K. (2017, May). Target to pay $18.5M for 2013 data breach that affected 41 million 
consumers. Retrieved from https://www.usatoday.com/story/money/2017/05/23/target-pay-185m-2013-data-breach-affected-consumers/102063932/
Zaharia, M., & Xin, R. S. (2016, November). Apache Spark: A Unified Engine for Big Data 
Processing. Communications of the ACM, 59(11), 56-65. Retrieved from ACM Digital Database
